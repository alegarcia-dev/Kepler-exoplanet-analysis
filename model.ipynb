{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a977cb1e-48bb-4c00-94dd-caae5c34e780",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "This notebook contains all steps and decisions in the modeling phase of the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## The Required Imports\n",
    "\n",
    "Below are all the modules needed to run the code cells in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c47424-7ed8-4812-bd76-d1b875d93ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from wrangle import wrangle_kepler_modeling\n",
    "from model import *\n",
    "from preprocessing import *\n",
    "from baseline import establish_classification_baseline\n",
    "from evaluate import *\n",
    "\n",
    "# We'll be using this random seed throughout.\n",
    "random_seed = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b826fa3-5b1b-4aad-aad0-d8ce0bcb0026",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Acquire, Prepare, and Split the Data\n",
    "\n",
    "Let's acquire, prepare, and split our data using the wrangle module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18877b7f-397e-438f-8702-8f42aa1516f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3376, 7), (1448, 7), (1207, 7))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, validate, test = wrangle_kepler_modeling()\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdf5149-3f05-48d3-ac2e-824ba6eafd15",
   "metadata": {},
   "source": [
    "Let's also scale our data while we're at it because chances are we're going to need the data to be scaled considering some features have extremely large values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6eae13-12ec-4dd1-a2f3-23042439dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled, validate_scaled, test_scaled = scale_data(\n",
    "    train,\n",
    "    validate,\n",
    "    test,\n",
    "    train.drop(columns = 'disposition').columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a82408-bb20-4cd7-ae24-43fbca85a4bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A Note on Evaluation\n",
    "\n",
    "In this project we will be primarily be using accuracy to measure the performance of our models since there is a fairly even split between false positives and confirmed exoplanets. However, we will also make note of the recall scores in comparing model performance. In the case when two models have similar accuracy the precision score will help break the tie since we want to be sure of our positive predictions. When an observation is predicted as a confirmed exoplanet this would reasonably lead to prioritizing analysis of that object to verify the prediction. We want to be sure that this time spent analyzing an object will not be for nothing.\n",
    "\n",
    "Additionally, since we are interested primarily in identifying confirmed exoplanets more than false positive dispositions, the disposition of CONFIRMED will be our positive case and FALSE POSITIVE will be the negative (that's going to get confusing).\n",
    "\n",
    "## Establish a Baseline\n",
    "\n",
    "Before we can begin building models we have to establish a baseline model to compare our models to. This way we can determine if our models at least perform better than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414cd325-9bc3-431f-afce-8f5272d2ae30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3376,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = establish_classification_baseline(train.disposition)\n",
    "baseline.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b8c8f4-ac08-439c-bef9-18e46a354d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FALSE POSITIVE    3376\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "423a5803-3acc-4924-a3ed-f550358b355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's make a baseline model for validate.\n",
    "# We'll use FALSE POSITIVE as the value the baseline model predicts because that was the value chosen \n",
    "# for the train baseline.\n",
    "validate_baseline = pd.Series('FALSE POSITIVE', index = validate.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d1a539-2058-4d98-8613-cb411ce89e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FALSE POSITIVE    1448\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_baseline.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615fdcf-e84d-47af-8cf9-a2af7523483b",
   "metadata": {},
   "source": [
    "Here the baseline is a model that always predicts an observation is a false positive since that is the most common value in the target variable. Let's evaluate the baseline model's performance before we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89316544-4167-4181-bcfc-59914bd1cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    **evaluate(train.disposition, baseline, 'CONFIRMED', prefix = 'train_'),\n",
    "    **evaluate(validate.disposition, validate_baseline, 'CONFIRMED', prefix = 'validate_')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef505de1-6a4c-4e39-afb2-46be0ce67f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>validate_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.624111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.623619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          train_accuracy  train_recall  train_precision  validate_accuracy  \\\n",
       "Baseline        0.624111           0.0              0.0           0.623619   \n",
       "\n",
       "          validate_recall  validate_precision  \n",
       "Baseline              0.0                 0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = append_results('Baseline', results)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80b4a8-ae05-49a1-8161-538be4a60869",
   "metadata": {},
   "source": [
    "So our baseline has accuracy of 62% on both train and validate, and 0 for both recall and precision (which is expected).\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "Let's now use RFE to rank our features so that we can start with a few and add more features in the order they are ranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc4512c-1dfe-442c-ae46-84e2b9ff403f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=DecisionTreeClassifier(max_depth=3), n_features_to_select=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe = RFE(DecisionTreeClassifier(max_depth = 3), n_features_to_select = 2)\n",
    "rfe.fit(train_scaled.drop(columns = 'disposition'), train_scaled.disposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e87a400-58ad-4ed4-8f75-aa5685291c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transit_depth</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>planetary_radius</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>temperature</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>normalized_depth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>orbital_period</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>transit_duration</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Var  Rank\n",
       "0     transit_depth     3\n",
       "1  planetary_radius     1\n",
       "2       temperature     5\n",
       "3  normalized_depth     2\n",
       "4    orbital_period     1\n",
       "5  transit_duration     4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Var': train_scaled.drop(columns = 'disposition').columns, 'Rank': rfe.ranking_})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd2cbd-827a-4839-9cfe-0efa26a37131",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create Some Models\n",
    "\n",
    "Now let's finally create some models to predict the exoplanet archive disposition. We'll start by creating a variety of models using different algorithms and our top two features chosen by RFE. For whichever provides the best performance we'll try a variety of different hyper-parameters and start adding on additional features and then choose our best model from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68f2e77f-5b05-410c-ba06-e2ee69a4b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\n",
    "    DecisionTreeClassifier(max_depth = 3, random_state = random_seed),\n",
    "    RandomForestClassifier(max_depth = 3, random_state = random_seed),\n",
    "    AdaBoostClassifier(random_state = random_seed),\n",
    "    BaggingClassifier(random_state = random_seed),\n",
    "    GradientBoostingClassifier(random_state = random_seed),\n",
    "    KNeighborsClassifier(),\n",
    "    SGDClassifier(random_state = random_seed),\n",
    "    BernoulliNB(),\n",
    "    SVC(random_state = random_seed)\n",
    "]\n",
    "\n",
    "features = ['planetary_radius', 'orbital_period']\n",
    "models = {}\n",
    "\n",
    "for key in range(1, len(algorithms) + 1):\n",
    "    models[key] = Model(algorithms[key - 1], train, features, 'disposition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3ebffc6-f099-49fb-8bca-a1f96f9ce075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: <model.Model at 0x7fa8711b6dc0>,\n",
       " 2: <model.Model at 0x7fa880e04a30>,\n",
       " 3: <model.Model at 0x7fa8711b68b0>,\n",
       " 4: <model.Model at 0x7fa880e43f10>,\n",
       " 5: <model.Model at 0x7fa88209bf70>,\n",
       " 6: <model.Model at 0x7fa880de6c40>,\n",
       " 7: <model.Model at 0x7fa880e046d0>,\n",
       " 8: <model.Model at 0x7fa880df8700>,\n",
       " 9: <model.Model at 0x7fa870b12160>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "175919fa-31f9-4e7a-bdc3-6484a4b3b265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>validate_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.624111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.841825</td>\n",
       "      <td>0.901497</td>\n",
       "      <td>0.736639</td>\n",
       "      <td>0.832873</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.730594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.846564</td>\n",
       "      <td>0.878645</td>\n",
       "      <td>0.753888</td>\n",
       "      <td>0.830801</td>\n",
       "      <td>0.833028</td>\n",
       "      <td>0.746711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ada Boost</th>\n",
       "      <td>0.846564</td>\n",
       "      <td>0.875493</td>\n",
       "      <td>0.755269</td>\n",
       "      <td>0.828039</td>\n",
       "      <td>0.838532</td>\n",
       "      <td>0.739482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier</th>\n",
       "      <td>0.986374</td>\n",
       "      <td>0.988180</td>\n",
       "      <td>0.975875</td>\n",
       "      <td>0.806630</td>\n",
       "      <td>0.766972</td>\n",
       "      <td>0.732049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.872334</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.806735</td>\n",
       "      <td>0.841160</td>\n",
       "      <td>0.803670</td>\n",
       "      <td>0.780749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.878851</td>\n",
       "      <td>0.866036</td>\n",
       "      <td>0.821375</td>\n",
       "      <td>0.822514</td>\n",
       "      <td>0.779817</td>\n",
       "      <td>0.756228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>0.525474</td>\n",
       "      <td>0.116627</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.537983</td>\n",
       "      <td>0.150459</td>\n",
       "      <td>0.284722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli NB</th>\n",
       "      <td>0.624111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.730154</td>\n",
       "      <td>0.946414</td>\n",
       "      <td>0.587573</td>\n",
       "      <td>0.729972</td>\n",
       "      <td>0.950459</td>\n",
       "      <td>0.587302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    train_accuracy  train_recall  train_precision  \\\n",
       "Baseline                  0.624111      0.000000         0.000000   \n",
       "Decision Tree             0.841825      0.901497         0.736639   \n",
       "Random Forest             0.846564      0.878645         0.753888   \n",
       "Ada Boost                 0.846564      0.875493         0.755269   \n",
       "Bagging Classifier        0.986374      0.988180         0.975875   \n",
       "Gradient Boosting         0.872334      0.868400         0.806735   \n",
       "KNN                       0.878851      0.866036         0.821375   \n",
       "SGD                       0.525474      0.116627         0.235294   \n",
       "Bernoulli NB              0.624111      0.000000         0.000000   \n",
       "SVC                       0.730154      0.946414         0.587573   \n",
       "\n",
       "                    validate_accuracy  validate_recall  validate_precision  \n",
       "Baseline                     0.623619         0.000000            0.000000  \n",
       "Decision Tree                0.832873         0.880734            0.730594  \n",
       "Random Forest                0.830801         0.833028            0.746711  \n",
       "Ada Boost                    0.828039         0.838532            0.739482  \n",
       "Bagging Classifier           0.806630         0.766972            0.732049  \n",
       "Gradient Boosting            0.841160         0.803670            0.780749  \n",
       "KNN                          0.822514         0.779817            0.756228  \n",
       "SGD                          0.537983         0.150459            0.284722  \n",
       "Bernoulli NB                 0.623619         0.000000            0.000000  \n",
       "SVC                          0.729972         0.950459            0.587302  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [\n",
    "    'Decision Tree',\n",
    "    'Random Forest',\n",
    "    'Ada Boost',\n",
    "    'Bagging Classifier',\n",
    "    'Gradient Boosting',\n",
    "    'KNN',\n",
    "    'SGD',\n",
    "    'Bernoulli NB',\n",
    "    'SVC'\n",
    "]\n",
    "\n",
    "for model, name in zip(models.values(), names):\n",
    "    eval_df = append_results(\n",
    "        name,\n",
    "        {\n",
    "            **evaluate(train.disposition, model.make_predictions(train), 'CONFIRMED', prefix = 'train_'),\n",
    "            **evaluate(validate.disposition, model.make_predictions(validate), 'CONFIRMED', prefix = 'validate_')\n",
    "        },\n",
    "        eval_df\n",
    "    )\n",
    "    \n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1225d89-802d-4ea7-af68-0488919f0c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>validate_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.872334</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.806735</td>\n",
       "      <td>0.841160</td>\n",
       "      <td>0.803670</td>\n",
       "      <td>0.780749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.841825</td>\n",
       "      <td>0.901497</td>\n",
       "      <td>0.736639</td>\n",
       "      <td>0.832873</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.730594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.846564</td>\n",
       "      <td>0.878645</td>\n",
       "      <td>0.753888</td>\n",
       "      <td>0.830801</td>\n",
       "      <td>0.833028</td>\n",
       "      <td>0.746711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ada Boost</th>\n",
       "      <td>0.846564</td>\n",
       "      <td>0.875493</td>\n",
       "      <td>0.755269</td>\n",
       "      <td>0.828039</td>\n",
       "      <td>0.838532</td>\n",
       "      <td>0.739482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.878851</td>\n",
       "      <td>0.866036</td>\n",
       "      <td>0.821375</td>\n",
       "      <td>0.822514</td>\n",
       "      <td>0.779817</td>\n",
       "      <td>0.756228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier</th>\n",
       "      <td>0.986374</td>\n",
       "      <td>0.988180</td>\n",
       "      <td>0.975875</td>\n",
       "      <td>0.806630</td>\n",
       "      <td>0.766972</td>\n",
       "      <td>0.732049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.730154</td>\n",
       "      <td>0.946414</td>\n",
       "      <td>0.587573</td>\n",
       "      <td>0.729972</td>\n",
       "      <td>0.950459</td>\n",
       "      <td>0.587302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.624111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli NB</th>\n",
       "      <td>0.624111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>0.525474</td>\n",
       "      <td>0.116627</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.537983</td>\n",
       "      <td>0.150459</td>\n",
       "      <td>0.284722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    train_accuracy  train_recall  train_precision  \\\n",
       "Gradient Boosting         0.872334      0.868400         0.806735   \n",
       "Decision Tree             0.841825      0.901497         0.736639   \n",
       "Random Forest             0.846564      0.878645         0.753888   \n",
       "Ada Boost                 0.846564      0.875493         0.755269   \n",
       "KNN                       0.878851      0.866036         0.821375   \n",
       "Bagging Classifier        0.986374      0.988180         0.975875   \n",
       "SVC                       0.730154      0.946414         0.587573   \n",
       "Baseline                  0.624111      0.000000         0.000000   \n",
       "Bernoulli NB              0.624111      0.000000         0.000000   \n",
       "SGD                       0.525474      0.116627         0.235294   \n",
       "\n",
       "                    validate_accuracy  validate_recall  validate_precision  \n",
       "Gradient Boosting            0.841160         0.803670            0.780749  \n",
       "Decision Tree                0.832873         0.880734            0.730594  \n",
       "Random Forest                0.830801         0.833028            0.746711  \n",
       "Ada Boost                    0.828039         0.838532            0.739482  \n",
       "KNN                          0.822514         0.779817            0.756228  \n",
       "Bagging Classifier           0.806630         0.766972            0.732049  \n",
       "SVC                          0.729972         0.950459            0.587302  \n",
       "Baseline                     0.623619         0.000000            0.000000  \n",
       "Bernoulli NB                 0.623619         0.000000            0.000000  \n",
       "SGD                          0.537983         0.150459            0.284722  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.sort_values(by = 'validate_accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20070c8c-e261-4d1a-8237-633e3a13af61",
   "metadata": {},
   "source": [
    "Considering the performance on unseen data, Gradient Boosting has the best performance, and also not too much of a performance drop off from the train set either which is good news. We'll move forward with this algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "## Modifying the Hyper-Parameters\n",
    "\n",
    "Now let's try changing the hyper-parameters for gradient boosting to see if we can get better results that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddd496d5-b553-46bb-9c45-4a493746dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use loops to try a variety of hyper-parameters\n",
    "\n",
    "GBC_models = {}\n",
    "n = count()\n",
    "\n",
    "for loss in ['deviance', 'exponential']:\n",
    "    for n_estimators in range(100, 501, 50):\n",
    "        for max_depth in range(3, 11):\n",
    "            GBC_models[next(n)] = Model(\n",
    "                GradientBoostingClassifier(\n",
    "                    loss = loss,\n",
    "                    n_estimators = n_estimators,\n",
    "                    max_depth = max_depth,\n",
    "                    random_state = random_seed\n",
    "                ),\n",
    "                train,\n",
    "                features,\n",
    "                'disposition'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03d0b83a-fdb1-4af7-90f7-4bce3fae8860",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, model in enumerate(GBC_models.values()):\n",
    "    eval_df = append_results(\n",
    "        f'GBC_{index}',\n",
    "        {\n",
    "            **evaluate(train.disposition, model.make_predictions(train), 'CONFIRMED', prefix = 'train_'),\n",
    "            **evaluate(validate.disposition, model.make_predictions(validate), 'CONFIRMED', prefix = 'validate_')\n",
    "        },\n",
    "        eval_df\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5a8c827-135b-4681-86f5-9a64a80cc7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>validate_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GBC_104</th>\n",
       "      <td>0.894254</td>\n",
       "      <td>0.897557</td>\n",
       "      <td>0.833821</td>\n",
       "      <td>0.845304</td>\n",
       "      <td>0.811009</td>\n",
       "      <td>0.785080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_40</th>\n",
       "      <td>0.917654</td>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.867309</td>\n",
       "      <td>0.843232</td>\n",
       "      <td>0.796330</td>\n",
       "      <td>0.789091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_89</th>\n",
       "      <td>0.908175</td>\n",
       "      <td>0.912530</td>\n",
       "      <td>0.853353</td>\n",
       "      <td>0.842541</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.782531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_112</th>\n",
       "      <td>0.903436</td>\n",
       "      <td>0.907013</td>\n",
       "      <td>0.846946</td>\n",
       "      <td>0.841851</td>\n",
       "      <td>0.801835</td>\n",
       "      <td>0.783154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_24</th>\n",
       "      <td>0.902844</td>\n",
       "      <td>0.900709</td>\n",
       "      <td>0.849814</td>\n",
       "      <td>0.841160</td>\n",
       "      <td>0.794495</td>\n",
       "      <td>0.785844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Classifier</th>\n",
       "      <td>0.986374</td>\n",
       "      <td>0.988180</td>\n",
       "      <td>0.975875</td>\n",
       "      <td>0.806630</td>\n",
       "      <td>0.766972</td>\n",
       "      <td>0.732049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.730154</td>\n",
       "      <td>0.946414</td>\n",
       "      <td>0.587573</td>\n",
       "      <td>0.729972</td>\n",
       "      <td>0.950459</td>\n",
       "      <td>0.587302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli NB</th>\n",
       "      <td>0.624111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.624111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>0.525474</td>\n",
       "      <td>0.116627</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.537983</td>\n",
       "      <td>0.150459</td>\n",
       "      <td>0.284722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    train_accuracy  train_recall  train_precision  \\\n",
       "GBC_104                   0.894254      0.897557         0.833821   \n",
       "GBC_40                    0.917654      0.921986         0.867309   \n",
       "GBC_89                    0.908175      0.912530         0.853353   \n",
       "GBC_112                   0.903436      0.907013         0.846946   \n",
       "GBC_24                    0.902844      0.900709         0.849814   \n",
       "...                            ...           ...              ...   \n",
       "Bagging Classifier        0.986374      0.988180         0.975875   \n",
       "SVC                       0.730154      0.946414         0.587573   \n",
       "Bernoulli NB              0.624111      0.000000         0.000000   \n",
       "Baseline                  0.624111      0.000000         0.000000   \n",
       "SGD                       0.525474      0.116627         0.235294   \n",
       "\n",
       "                    validate_accuracy  validate_recall  validate_precision  \n",
       "GBC_104                      0.845304         0.811009            0.785080  \n",
       "GBC_40                       0.843232         0.796330            0.789091  \n",
       "GBC_89                       0.842541         0.805505            0.782531  \n",
       "GBC_112                      0.841851         0.801835            0.783154  \n",
       "GBC_24                       0.841160         0.794495            0.785844  \n",
       "...                               ...              ...                 ...  \n",
       "Bagging Classifier           0.806630         0.766972            0.732049  \n",
       "SVC                          0.729972         0.950459            0.587302  \n",
       "Bernoulli NB                 0.623619         0.000000            0.000000  \n",
       "Baseline                     0.623619         0.000000            0.000000  \n",
       "SGD                          0.537983         0.150459            0.284722  \n",
       "\n",
       "[154 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.sort_values(by = 'validate_accuracy', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce38b64a-b6ac-4e8b-82e9-6e105bcd930c",
   "metadata": {},
   "source": [
    "It looks like GBC_104 has a slightly better performance than the plain GBC algorithm. Let's see what the hyper-parameters were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36283e51-a171-435b-a03f-bcd49203988d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(loss='exponential', n_estimators=300,\n",
       "                           random_state=24)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBC_models[104].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b77df5e-5575-4f09-90b2-ec54e7f4e2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validate_accuracy': 0.8453038674033149,\n",
       " 'validate_recall': 0.8110091743119267,\n",
       " 'validate_precision': 0.7850799289520426}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's just verify this is the right one.\n",
    "evaluate(validate.disposition, GBC_models[104].make_predictions(validate), 'CONFIRMED', prefix = 'validate_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf0eb5-7fe4-47c0-9752-29bd913e0464",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Adding More Features\n",
    "\n",
    "Finally, let's progressively add more features to the model to see if adding more features improves our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf65611c-64eb-4b04-8532-53848e6551ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will define our features in the order RFE ranked them.\n",
    "features = [\n",
    "    'planetary_radius',\n",
    "    'orbital_period',\n",
    "    'normalized_depth',\n",
    "    'transit_depth',\n",
    "    'transit_duration',\n",
    "    'temperature'\n",
    "]\n",
    "\n",
    "GBC_models_more_features = {}\n",
    "\n",
    "# Now we'll add features one at a time and produce models along the way.\n",
    "# We start with 3 since we already have a model with 2 features.\n",
    "for n in range(3, len(features) + 1):\n",
    "    GBC_models_more_features[f'GBC_104_{n}_features'] = Model(\n",
    "        GradientBoostingClassifier(\n",
    "            loss = 'exponential',\n",
    "            n_estimators = 300,\n",
    "            random_state = random_seed\n",
    "        ),\n",
    "        train,\n",
    "        features[ : n],\n",
    "        'disposition'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cf1d813-621d-4bdf-8f46-e885864f9035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we evaluate.\n",
    "for index, model in enumerate(GBC_models_more_features.values()):\n",
    "    eval_df = append_results(\n",
    "        f'GBC_104_{index + 3}_features',\n",
    "        {\n",
    "            **evaluate(train.disposition, model.make_predictions(train), 'CONFIRMED', prefix = 'train_'),\n",
    "            **evaluate(validate.disposition, model.make_predictions(validate), 'CONFIRMED', prefix = 'validate_')\n",
    "        },\n",
    "        eval_df\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5db963c1-18a7-4b32-a4d1-233e10a19c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>validate_recall</th>\n",
       "      <th>validate_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GBC_104_6_features</th>\n",
       "      <td>0.954976</td>\n",
       "      <td>0.956659</td>\n",
       "      <td>0.926011</td>\n",
       "      <td>0.899862</td>\n",
       "      <td>0.889908</td>\n",
       "      <td>0.850877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_104_5_features</th>\n",
       "      <td>0.949348</td>\n",
       "      <td>0.952719</td>\n",
       "      <td>0.915909</td>\n",
       "      <td>0.895028</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.846561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_104_4_features</th>\n",
       "      <td>0.928318</td>\n",
       "      <td>0.933018</td>\n",
       "      <td>0.882923</td>\n",
       "      <td>0.883287</td>\n",
       "      <td>0.884404</td>\n",
       "      <td>0.819728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_104_3_features</th>\n",
       "      <td>0.922393</td>\n",
       "      <td>0.930654</td>\n",
       "      <td>0.871587</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.856881</td>\n",
       "      <td>0.819298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_104</th>\n",
       "      <td>0.894254</td>\n",
       "      <td>0.897557</td>\n",
       "      <td>0.833821</td>\n",
       "      <td>0.845304</td>\n",
       "      <td>0.811009</td>\n",
       "      <td>0.785080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_40</th>\n",
       "      <td>0.917654</td>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.867309</td>\n",
       "      <td>0.843232</td>\n",
       "      <td>0.796330</td>\n",
       "      <td>0.789091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_89</th>\n",
       "      <td>0.908175</td>\n",
       "      <td>0.912530</td>\n",
       "      <td>0.853353</td>\n",
       "      <td>0.842541</td>\n",
       "      <td>0.805505</td>\n",
       "      <td>0.782531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_112</th>\n",
       "      <td>0.903436</td>\n",
       "      <td>0.907013</td>\n",
       "      <td>0.846946</td>\n",
       "      <td>0.841851</td>\n",
       "      <td>0.801835</td>\n",
       "      <td>0.783154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.872334</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.806735</td>\n",
       "      <td>0.841160</td>\n",
       "      <td>0.803670</td>\n",
       "      <td>0.780749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC_82</th>\n",
       "      <td>0.922393</td>\n",
       "      <td>0.932230</td>\n",
       "      <td>0.870493</td>\n",
       "      <td>0.841160</td>\n",
       "      <td>0.794495</td>\n",
       "      <td>0.785844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    train_accuracy  train_recall  train_precision  \\\n",
       "GBC_104_6_features        0.954976      0.956659         0.926011   \n",
       "GBC_104_5_features        0.949348      0.952719         0.915909   \n",
       "GBC_104_4_features        0.928318      0.933018         0.882923   \n",
       "GBC_104_3_features        0.922393      0.930654         0.871587   \n",
       "GBC_104                   0.894254      0.897557         0.833821   \n",
       "GBC_40                    0.917654      0.921986         0.867309   \n",
       "GBC_89                    0.908175      0.912530         0.853353   \n",
       "GBC_112                   0.903436      0.907013         0.846946   \n",
       "Gradient Boosting         0.872334      0.868400         0.806735   \n",
       "GBC_82                    0.922393      0.932230         0.870493   \n",
       "\n",
       "                    validate_accuracy  validate_recall  validate_precision  \n",
       "GBC_104_6_features           0.899862         0.889908            0.850877  \n",
       "GBC_104_5_features           0.895028         0.880734            0.846561  \n",
       "GBC_104_4_features           0.883287         0.884404            0.819728  \n",
       "GBC_104_3_features           0.875000         0.856881            0.819298  \n",
       "GBC_104                      0.845304         0.811009            0.785080  \n",
       "GBC_40                       0.843232         0.796330            0.789091  \n",
       "GBC_89                       0.842541         0.805505            0.782531  \n",
       "GBC_112                      0.841851         0.801835            0.783154  \n",
       "Gradient Boosting            0.841160         0.803670            0.780749  \n",
       "GBC_82                       0.841160         0.794495            0.785844  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.sort_values(by = 'validate_accuracy', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c03bec3-bea6-423b-9a7c-83cc2a0cb307",
   "metadata": {},
   "source": [
    "It looks like adding more features does make a significant difference in performance. Adding in all 6 features greatly improves the performance from the model with just 2 features and also has slightly better performance than the model with 5 features.\n",
    "\n",
    "The drop off in performance between train and validate isn't too much either. A drop of about 5.5% in accuracy and 7.5% in precision is within an acceptable range, where an acceptable range would be between 1% and 10%. An accuracy of nearly 90% on unseen data is very good meaning the model misclassified only 10% of the data, or roughly 145 observations in validate.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluate on Test\n",
    "\n",
    "Finally let's use our best model to evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29a62c86-8b26-439d-a34d-6573895a4fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GBC_104_6_features</th>\n",
       "      <td>0.893123</td>\n",
       "      <td>0.909692</td>\n",
       "      <td>0.824351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    test_accuracy  test_recall  test_precision\n",
       "GBC_104_6_features       0.893123     0.909692        0.824351"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "append_results(\n",
    "    'GBC_104_6_features',\n",
    "    evaluate(\n",
    "        test.disposition,\n",
    "        GBC_models_more_features['GBC_104_6_features'].make_predictions(test),\n",
    "        positive_label = 'CONFIRMED',\n",
    "        prefix = 'test_'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3db66-d3f4-4677-b4f2-5fea7832339b",
   "metadata": {},
   "source": [
    "The results are fairly consistent with the results obtained for the validate set which means we do not have an overfit model.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "To conclude, we were able to produce a Gradient Boosting Classifier model with hyper-parameters \"exponential\" for loss, 300 for n_estimators, and a max_depth of 3 that has nearly 90% accuracy on unseen data which is roughly 27 percentage points better than the baseline. The features used in this model were planetary_radius, orbital_period, normalized_depth, transit_depth, transit_duration, and temperature.\n",
    "\n",
    "This model was compared to 157 other models using various algorithms, number of features, and hyper-parameters. The drop off in performance between the training set and unseen data was within an acceptable range indicating that the model is not overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
